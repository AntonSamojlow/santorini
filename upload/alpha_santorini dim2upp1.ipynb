{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Santorini zero v01 \n",
    "This notebook is a first attempt to implement the methods described in https://web.stanford.edu/~surag/posts/alphazero.html for Santorini. Since for small values of <i>board_dim</i> and <i>units_per_player</i>, an optimal policy $\\pi^*$ can easily be determined, we test whether the NN can compete with $\\pi^*$. Note that $\\pi^*$ might not be unique, but its performance is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the dimensions of the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---P--\n",
      "P0  0 \n",
      " 0 O0 \n",
      "------\n"
     ]
    }
   ],
   "source": [
    "import santorini as san\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "env = san.Environment(board_dim=2, units_per_player=1)\n",
    "\n",
    "if env.units_per_player == 1 and env.board_dim == 2: max_actions = 4\n",
    "else: max_actions = 64*env.units_per_player\n",
    "\n",
    "start_state_collection = { (2,1) : '00000011', \n",
    "                           (3,1) : '0000000000022', (3,2) : '00000000000022022',\n",
    "                           (4,1) : '00000000000000000033', (4,2) : '000000000000000000033033',\n",
    "                           (5,1) : '00000000000000000000000001133', (5,2) : '000000000000000000000000011133133' }                    \n",
    "\n",
    "s_start = start_state_collection[(env.board_dim, env.units_per_player)]\n",
    "san.State.from_string(s_start , board_dim=env.board_dim).print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks on Notation\n",
    "We identify actions of a state with the targetsstate since state-action transitions are deterministic. The symbol $s$ will denote the string-representation of a state, e.g. $s=$'00000011'. We therefore introduce some helper functions for the conversion to the following other representations: \n",
    "* an instance of the <i>class santorini.State</i> \n",
    "* a python-list of integers \n",
    "\n",
    "We also introduce the method <b>children(s)</b> that returns the childnodes of a state $s$ in their string-representation. <b>[!]Note:</b> The order in which they are returned defines the indexing of states, in particular for policy-vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state(s): return san.State.from_string(s, board_dim=env.board_dim)\n",
    "\n",
    "def children(s): return [env.do_play(p, state(s)).string() for p in env.get_plays(state(s))]\n",
    "\n",
    "def integers(s): return [int(c) for c in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphabeta\n",
    "negamax = alphabeta.alphabeta(env.score, env.get_plays, env.do_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the maximum depth\n",
    "t_max=0\n",
    "for t in range(0, 4*env.board_dim*env.board_dim):\n",
    "    if negamax.basic(state(s_start), t_max)[0] == 0: t_max+=1\n",
    "    else: break\n",
    "\n",
    "# populate keys: all non-terminal states\n",
    "pi_opt = { st.string() : 0 for st in state('00000011').equiv_class()}\n",
    "for t in range(0, t_max):\n",
    "    for s in list(pi_opt.keys()):  \n",
    "        for c in children(s):\n",
    "            if env.score(state(c)) == 0: pi_opt.update({c : 0})\n",
    "# populate values    \n",
    "for s in list(pi_opt.keys()): \n",
    "    v, play = negamax.basic(state(s), t_max)\n",
    "    pi_opt.update({s : [env.do_play(play, state(s)).string(), v]})\n",
    "\n",
    "# reformat output to match NN architecture\n",
    "def pi_opt_predict(s):\n",
    "    ss, v = pi_opt[s]\n",
    "    return [[1 if i==children(s).index(ss) else 0 for i in range(0,max_actions) ], v]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating training data\n",
    "The data for the training of the NN is gathered via self-play. A tree search (TS) traverses the game tree node by node by the following rules, starting from a root node: When a terminal node is encountered, its value ($\\pm1$) is returned and the TS terminates. When a so-far unvisited node $s$ is encountered (instead of a random rollout as a MCTS would do) the expected reward of the node is returned by calling the NN on $s$. Moreover, the TS ends in this case with $Q(s,\\cdot)$ and $N(s,\\cdot)$ initialised to zero and $P(s, \\cdot) \\leftarrow \\text{NN}(s)$. If the (non-terminal) node $s$ has been visited, then the search continues on the next node $s^\\prime$ chosen as the maximizer of the upper confidence bound $U(s,s^\\prime):=Q(s,s\\prime) + c_{UCB}P(s,s^\\prime)\\frac{\\sqrt{\\sum_{s^\\prime} N(s,s^\\prime)}}{N(s,s^\\prime)+1}$. Whenever the TS ends (unvisited or terminal node), the N and Q-values along the serach path are updated: Given an edge $ss^\\prime$, we increment $N(s,s^\\prime)$ by one and then apply the update rule $Q(s,s^\\prime) \\leftarrow \\frac{v-Q(s,s^\\prime)}{N(s,s^\\prime)}$.  \n",
    "\n",
    "Finally, after a fixed number of simulations (searchs), the TS returns an estaimte of the policy at the root node, which is $\\pi(s_{root},s^\\prime) \\propto N(s,s^\\prime)^\\tau $ for a given temperatur $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm: self_play($N_{TS}$):\n",
    "1. Initialise $s=s_{init}$,  a tree $T$ and a game log $L$.  \n",
    "2. While $s$ is not terminal, repeat:     \n",
    "    a) $N \\leftarrow$ TS($s,N_{TS}$)  \n",
    "    b) If $\\tau$ is given, $\\pi[s][s^\\prime] :=   (N[s][s^\\prime])^\\tau /( \\sum_{s^\\prime} (N[s][s^\\prime])^\\tau )$, else $\\pi[s][s^\\prime] := \\delta(s^\\prime - \\arg\\max N[s][\\cdot])$.  \n",
    "    c) Add $(s,\\pi[s], \\text{None} )$ to $L$.  \n",
    "    d) Sample the next $s$ from its children (stored in $T$) with probability $\\pi[s]$  .\n",
    "4. Propagate the game rewards (update the 3rd value for each $l\\in L$), depending on who won.\n",
    "5. <b>Return</b> $L$.\n",
    "\n",
    "#### Algorithm TS($s,N_{TS}$): \n",
    "1. Initialise $N,Q,P$ and the tree $T$.  \n",
    "2. Run the subroutine TS_run($s$) $N_{TS}$-times  \n",
    "3. <b>Return</b> $N$.  \n",
    "\n",
    "Subroutine TS_run($s$):\n",
    "1. If $s$ is terminal <b>return</b> -score($s$).\n",
    "2. If $s \\not \\in T$ then add it to $T$ (compute and store the children). \n",
    "3. If $s$ has not been visited yet:  \n",
    "    a) Initialise $Q[s][\\cdot ] = N[s][\\cdot] = 0$.  \n",
    "    b) Get $P[s][\\cdot], v[s] \\leftarrow NN[s]$.   \n",
    "    c) <b>Return</b> $-v$.\n",
    "4. Obtain $s^\\prime \\leftarrow \\arg \\max_{s^\\prime}U[s][s^\\prime]$.\n",
    "5. Do a recursive call $v \\leftarrow$  <b>TS_run</b>($s^\\prime$).\n",
    "6. update first $N[s][s^\\prime]\\;+\\!=1$ then $Q[s][s^\\prime]\\;+\\!= \\frac{v-Q[s][s^\\prime]}{N[s][s^\\prime]}$.\n",
    "7. <b>Return</b> $-v$.\n",
    "    \n",
    "Note that TS_run returns the negative value, since a value is always from the <i>active</i> players perspective, which for the calling level (we have a recursive method) is the opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS(s, NN_predict_fct, N_searches=100, UCB_cst=1.0, tree=None):\n",
    "    if tree==None: tree = {} \n",
    "    N, Q, P = {}, {}, {}\n",
    "    \n",
    "    def TS_run(s):\n",
    "        if env.score(state(s)) != 0: return -env.score(state(s))\n",
    "        if s not in tree.keys(): tree.update({s : children(s)})\n",
    "        if s not in N.keys():\n",
    "            Q[s], N[s] = {ss : 0 for ss in tree[s]}, {ss : 0 for ss in tree[s]}\n",
    "            P[s], val = NN_predict_fct(s)\n",
    "            return -val            \n",
    "        U = [Q[s][ss] + UCB_cst*P[s][tree[s].index(ss)]*np.sqrt(sum(N[s].values())/(N[s][ss] +1))  for ss in tree[s]]\n",
    "        ss = tree[s][np.argmax(U)]\n",
    "        val = TS_run(ss)\n",
    "        N[s][ss]+=1\n",
    "        Q[s][ss]+= (val - Q[s][ss])/N[s][ss]\n",
    "        return -val\n",
    "    \n",
    "    for n in range(0, N_searches): TS_run(s)\n",
    "    return N\n",
    "\n",
    "def self_play(NN_predict_fct, temp=None, N_treesearches=100, UCB_cst=1.0):\n",
    "    \"\"\"\n",
    "    temp = None: probability vectors pi[s] are deterministic, else choose temp > 0\n",
    "    \"\"\"\n",
    "    s = s_start\n",
    "    tree={}    \n",
    "    game_log = []\n",
    "    while env.score(state(s)) == 0:\n",
    "        N = TS(s, NN_predict_fct, N_searches=N_treesearches, UCB_cst=UCB_cst, tree=tree)\n",
    "        if temp == None:\n",
    "            pi = [1 if i == np.argmax(N[s]) else 0 for i in range(0,max_actions)]\n",
    "        else:\n",
    "            pi = [pow(N[s][ss], temp)/sum([pow(N[s][ss], temp) for ss in tree[s]])  for ss in tree[s]]\n",
    "            for a in range(0, max_actions - tree[s].__len__()): pi.append(0)\n",
    "        game_log += [[s, pi, None]]\n",
    "        s = np.random.choice(tree[s], p=pi[:tree[s].__len__()]) \n",
    "    \n",
    "    L = game_log.__len__()\n",
    "    reward = env.score(state(s))\n",
    "    for i in range(L-2, -1, -2): game_log[i][2] = reward\n",
    "    for i in range(L-1, -1, -2): game_log[i][2] = -1*reward\n",
    "    return game_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the TS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_rand_predict(s):    \n",
    "    C = children(s)\n",
    "    N_c = C.__len__() \n",
    "    policy = [1/N_c if i < N_c else 0 for i in range(0, max_actions)]\n",
    "    return [policy, 0]        \n",
    "\n",
    "def TS_test(s, NN_predict_fct, N_searches=1000, UCB_cst=1.0):\n",
    "    stats = TS(s, NN_predict_fct, N_searches=N_searches, UCB_cst=UCB_cst, tree=None)[s]\n",
    "    state(s).print()\n",
    "    print('* TS proabilities for children by visit count:')\n",
    "    for c in children(s):\n",
    "        state(c).print()\n",
    "        print( str(round(100*stats[c]/sum(stats.values()), 1)),'%')\n",
    "        print('')\n",
    "\n",
    "\n",
    "def compare(predict_fct_1, predict_fct_2, N_games=500):\n",
    "    '''\n",
    "    Returns the win-percentage of predict_fct_1. Starting players alternate.\n",
    "    '''\n",
    "    N_wins = 0\n",
    "    for n in tqdm_notebook(range(0,N_games), desc='test games', leave=False):\n",
    "        s = s_start\n",
    "        turns_played = 0\n",
    "        while env.score(state(s)) == 0:\n",
    "            C = children(s)\n",
    "            if turns_played%2 == n%2: pi = predict_fct_1(s)[0][:C.__len__()]\n",
    "            else: pi = predict_fct_2(s)[0][:C.__len__()]\n",
    "            s = np.random.choice(C, p = [ p/(sum(pi)) for p in pi])\n",
    "            turns_played+=1            \n",
    "        N_wins += int(turns_played%2 == n%2 and env.score(state(s)) == 1)\\\n",
    "                    +int(turns_played%2 != n%2 and env.score(state(s)) == -1)          \n",
    "    return N_wins/N_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.928"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(pi_opt_predict, pi_rand_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---P--\n",
      "P2  3 \n",
      "O2  2 \n",
      "------\n",
      "* TS proabilities for children by visit count:\n",
      "---P--\n",
      " 3 O3 \n",
      "P2  2 \n",
      "------\n",
      "33.3 %\n",
      "\n",
      "---P--\n",
      " 2 O3 \n",
      "P2  3 \n",
      "------\n",
      "33.2 %\n",
      "\n",
      "---P--\n",
      " 3  3 \n",
      "P2 O2 \n",
      "------\n",
      "1.1 %\n",
      "\n",
      "---P--\n",
      " 2  4 \n",
      "P2 O2 \n",
      "------\n",
      "32.3 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TS_test('23220010', pi_rand_predict, N_searches=1000, UCB_cst=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NN and its policy iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import*\n",
    "\n",
    "class NeuralNet():\n",
    "    def __init__(self, env):\n",
    "        input_dim = env.board_dim*env.board_dim+4*env.units_per_player\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        input_states = keras.Input(shape=(input_dim,))\n",
    "        layer1 = Dense(64, activation='softmax')(input_states)\n",
    "        layer2 = Dense(256, activation='softmax')(layer1)\n",
    "        layer3 = Dense(128, activation='softmax')(layer2)\n",
    "        layer4 = Dense(64, activation='softmax')(layer3)\n",
    "        pi = Dense(max_actions, activation='softmax', name='pi')(layer4)  \n",
    "        v = Dense(1, activation='sigmoid', name='v')(layer1)                    \n",
    "        self.model = keras.models.Model(inputs=input_states, outputs=[pi, v])\n",
    "        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], \n",
    "                           optimizer=keras.optimizers.Adam(learning_rate))\n",
    "        \n",
    "    def info(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    def train(self, examples, epochs=100, verbose=False):\n",
    "        s, pi, v = [], [], []\n",
    "        for ex in examples:\n",
    "            s.append(integers(ex[0]))\n",
    "            pi.append(np.array(ex[1]))\n",
    "            v.append(ex[2])\n",
    "        x, y = np.array(s), [np.array(pi), np.array(v)]\n",
    "        self.model.fit(x, y, epochs=epochs, verbose=int(verbose))\n",
    "    \n",
    "    def predict(self, s):\n",
    "        pi, v = self.model.predict(np.array([integers(s)]))\n",
    "        return pi[0], v[0]\n",
    "    \n",
    "    def detailed_predict(self, s):\n",
    "        p, v = self.predict(s) \n",
    "        C = children(s)\n",
    "        state(s).print()\n",
    "        print('* v:',v[0])\n",
    "        print('* NN predictions for children:')\n",
    "        for i in range(0, C.__len__()):\n",
    "            state(C[i]).print()\n",
    "            print(p[i] ,'%')\n",
    "            print('')\n",
    "    \n",
    "    def play_vs(self, other, N_games=500):\n",
    "        return compare(self.predict, other.predict, N_games=N_games)     \n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def self_improve(self, threshold=0.55, train_data_size=100 ,N_treesearches=100, temp=1, UCB_cst=1.0, \n",
    "                     train_epochs=100, stagnation_tol=0.03, N_benchmark_games=500):\n",
    "        self.save('./test_oldNN.h5')\n",
    "        \n",
    "        oldNN = NeuralNet(env)\n",
    "        oldNN.load('./test_oldNN.h5')\n",
    "        \n",
    "        win_percentage = [0]\n",
    "        train_data = [] \n",
    "        for _ in tqdm_notebook(range(0, train_data_size), desc='selfplays', leave=False):\n",
    "            train_data += self_play(self.predict, temp=temp, N_treesearches=N_treesearches, UCB_cst=UCB_cst)\n",
    "        \n",
    "        # succesively try to train 10 times, unless stagnation or the goal threshold is reached\n",
    "        train_iterator = tqdm_notebook(range(0, 10), desc='train round', leave=False)\n",
    "        for t in train_iterator:\n",
    "            self.train(train_data, epochs=train_epochs)\n",
    "            win_percentage += [self.play_vs(oldNN, N_games=N_benchmark_games)]                \n",
    "            if (t > 0 and win_percentage[-1] - win_percentage[-2] < -stagnation_tol) or win_percentage[-1]>=threshold: \n",
    "                train_iterator.close()\n",
    "                break\n",
    "        print('win %:',win_percentage[:])\n",
    "        if  win_percentage[-1]>=threshold: \n",
    "            self.save('./test_newNN.h5')\n",
    "            print('* new weights saved to ./test_newNN.h5')\n",
    "        else: \n",
    "            self.load('./test_oldNN.h5')\n",
    "            print('* old weights restored')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_73 (InputLayer)           (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_288 (Dense)               (None, 64)           576         input_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_289 (Dense)               (None, 256)          16640       dense_288[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_290 (Dense)               (None, 128)          32896       dense_289[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_291 (Dense)               (None, 64)           8256        dense_290[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 4)            260         dense_291[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "v (Dense)                       (None, 1)            65          dense_288[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 58,693\n",
      "Trainable params: 58,693\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(env)\n",
    "nn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='selfplays', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb7fee616e24dea94e443a6a311c7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train round', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last win %: [0.478, 0.503, 0.527, 0.561]\n"
     ]
    }
   ],
   "source": [
    "nn.self_improve(threshold=0.55, train_data_size=50, N_treesearches=1000, temp=1, UCB_cst=0.5,\n",
    "                train_epochs=100, stagnation_tol=0.03, N_benchmark_games=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained an improvemed NN. Let's pit it against $\\pi_{rand}$ and $\\pi^*$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5534"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(nn.predict, pi_rand_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2391"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(nn.predict, pi_opt_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0599"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(pi_rand_predict, pi_opt_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a closer look at the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---P--\n",
      "P2  3 \n",
      "O2  2 \n",
      "------\n",
      "* v: 0.8320611\n",
      "* NN predictions for children:\n",
      "---P--\n",
      " 3 O3 \n",
      "P2  2 \n",
      "------\n",
      "0.9631542 %\n",
      "\n",
      "---P--\n",
      " 2 O3 \n",
      "P2  3 \n",
      "------\n",
      "0.036464352 %\n",
      "\n",
      "---P--\n",
      " 3  3 \n",
      "P2 O2 \n",
      "------\n",
      "0.00011417707 %\n",
      "\n",
      "---P--\n",
      " 2  4 \n",
      "P2 O2 \n",
      "------\n",
      "0.00026730486 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn.detailed_predict('23220010')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep self-improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='selfplays', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff6551f8ae4d61a444bfed03cf8da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train round', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last win %: [0.526, 0.533, 0.574]\n"
     ]
    }
   ],
   "source": [
    "nn.self_improve(threshold=0.55, train_data_size=50, N_treesearches=1000, temp=1, UCB_cst=0.8,\n",
    "                train_epochs=100, stagnation_tol=0.03, N_benchmark_games=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6157"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(nn.predict, pi_rand_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(nn.predict, pi_opt_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.save('./44pct_bdim2_upp1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='selfplays', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85df26d7435045498b4ddc5eb8044239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train round', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last win %: [0.517, 0.536, 0.54, 0.543, 0.544, 0.536, 0.562]\n"
     ]
    }
   ],
   "source": [
    "nn.self_improve(threshold=0.55, train_data_size=50, N_treesearches=1000, temp=1, UCB_cst=0.8,\n",
    "                train_epochs=100, stagnation_tol=0.03, N_benchmark_games=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4979"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(nn.predict, pi_opt_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.save('./49pct_bdim2_upp1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to self-improve one more time, lowering the threshold as we are close to $\\pi^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='selfplays'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618ca1454c1c4ba8b9b40ae61e7a1bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train round', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last win %: [0.511, 0.523]\n"
     ]
    }
   ],
   "source": [
    "nn.self_improve(threshold=0.52, train_data_size=100, N_treesearches=1000, temp=1, UCB_cst=1.0,\n",
    "                train_epochs=100, stagnation_tol=0.03, N_benchmark_games=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.499"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(nn.predict, pi_opt_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.save('./49_99pct_bdim2_upp1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6904"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(nn.predict, pi_rand_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test games', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9352"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(pi_opt_predict, pi_rand_predict, N_games=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We succesfully trained a NN to win against an optimal policy about 49,9 percent of the games. Note though, that against a random policy, the NN performs significantly worse thatn the optimal policy. This indicates that it found good strategies to beat expert level play, but not random plays..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
